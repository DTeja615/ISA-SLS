{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "import yaml\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from bottleneck import move_mean\n",
    "from gym import logger\n",
    "from gym.wrappers import Monitor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        # Running configuration\n",
    "        self.run = 0\n",
    "        self.step = 0\n",
    "        self.episode = 0\n",
    "        self.episode_count = config['EPISODES']\n",
    "\n",
    "        # Env\n",
    "        self.env_id = config['ENV_ID']\n",
    "        self.env_seed = config['ENV_SEED']\n",
    "        if config['VERBOSE'] > 0:\n",
    "            self.env = Monitor(gym.make(self.env_id), directory=config['RECORD_DIR'] + f'run_{self.run}',\n",
    "                               video_callable=lambda episode_id: episode_id % config['SAVE_EVERY'] == 0,\n",
    "                               force=True,\n",
    "                               uid=config['AGENT'])  # record every nth episode, clear monitor files if present\n",
    "        else:\n",
    "            self.env = gym.make(self.env_id)\n",
    "        self.env.seed(self.env_seed)\n",
    "\n",
    "        # Get random number generator\n",
    "        self.prng = np.random.RandomState(self.env_seed)\n",
    "\n",
    "        # Score/rewards over time\n",
    "        # Deque allows quick appends and pops and has a max length\n",
    "        self.score = deque(maxlen=self.episode_count)\n",
    "        self.score_100 = deque(maxlen=100)  # for keeping track of mean of last 100\n",
    "\n",
    "    def act(self, *args):\n",
    "        \n",
    "        # random action\n",
    "        return self.prng.randint(self.env.action_space.n)\n",
    "\n",
    "    def do_episode(self, config):\n",
    "\n",
    "        # Initial values\n",
    "        done = False\n",
    "        score_e = 0\n",
    "        step_e = 0\n",
    "\n",
    "        # Reset environment\n",
    "        self.env.reset()\n",
    "\n",
    "        # Continue while not crashed\n",
    "        while not done:\n",
    "\n",
    "            # Show on screen\n",
    "            if config['VERBOSE'] > 1:\n",
    "                self.env.render()\n",
    "\n",
    "            # Act\n",
    "            action = self.act()\n",
    "            _, reward, done, _ = self.env.step(action)\n",
    "\n",
    "            # Increment score and steps\n",
    "            score_e += reward\n",
    "            step_e += 1\n",
    "            self.step += 1\n",
    "\n",
    "        # Append score\n",
    "        self.score.append(score_e)\n",
    "        self.score_100.append(score_e)\n",
    "        mean_score = np.mean(self.score_100)\n",
    "\n",
    "        # Increment episode\n",
    "        self.episode += 1\n",
    "\n",
    "        if config['VERBOSE'] > 0:\n",
    "            logger.info(f'[Episode {self.episode}] - score: {score_e:.2f}, steps: {step_e}, '\n",
    "                        f'100-score: {mean_score:.2f}.')\n",
    "\n",
    "    def get_best_score(self):\n",
    "\n",
    "        # Best score is defined as highest 100-episode score reached (+ episode) when score < 200,\n",
    "        # or the episode when score >= 200\n",
    "        score_100 = move_mean(self.score, window=(100 if len(self.score) > 99 else len(self.score)), min_count=1)\n",
    "\n",
    "        # Get max\n",
    "        ep_max = np.argmax(score_100)\n",
    "        score_max = score_100[ep_max]\n",
    "\n",
    "        if score_max >= 200.0:\n",
    "            ep_max = np.argmax(score_100 >= 200.0)\n",
    "            score_max = 200.0  # to ensure equivalence\n",
    "\n",
    "        return int(ep_max), float(score_max)\n",
    "\n",
    "    def save_checkpoint(self, config):\n",
    "\n",
    "        # Env can't be saved\n",
    "        dummy_env = self.env\n",
    "        self.env = None\n",
    "\n",
    "        # Save checkpoint\n",
    "        with open(config['RECORD_DIR'] + 'checkpoint.pickle', 'wb') as p_file:\n",
    "            pickle.dump(self, p_file)\n",
    "\n",
    "        # Save config\n",
    "        with open(config['RECORD_DIR'] + 'config.json', 'w') as c_file:\n",
    "            json.dump(config, c_file, sort_keys=True, indent=4)\n",
    "\n",
    "        # Put env back\n",
    "        self.env = dummy_env\n",
    "        \n",
    "class SarsaAgent(RandomAgent):\n",
    "\n",
    "    # Agent that makes use of Sarsa (on-policy TD control).\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        # Initialize base class\n",
    "        super().__init__(config)\n",
    "\n",
    "        # State\n",
    "        self.state_bounds = config['STATE_BOUNDS']\n",
    "        self.state_bins = tuple(config['STATE_BINS'])\n",
    "\n",
    "        # Float conversion\n",
    "        for i, lr in enumerate(config['LEARNING_RATE']):\n",
    "            if type(lr) is str:\n",
    "                config['LEARNING_RATE'][i] = float(lr)\n",
    "        for i, eps in enumerate(config['E_GREEDY']):\n",
    "            if type(eps) is str:\n",
    "                config['E_GREEDY'][i] = float(eps)\n",
    "\n",
    "        # Learning parameters\n",
    "        # First linear decay, then exponential decay\n",
    "        self.alpha_start, self.alpha_end, self.alpha_steps, self.alpha_decay = config['LEARNING_RATE']\n",
    "        self.epsilon_start, self.epsilon_end, self.epsilon_steps, self.epsilon_decay = config['E_GREEDY']\n",
    "        self.alpha, self.epsilon = self.alpha_start, self.epsilon_start\n",
    "        self.gamma = float(config['DISCOUNT_RATE'])\n",
    "\n",
    "        # Q-table\n",
    "        self.q_table = self.prng.uniform(low=-1.0, high=1.0, size=self.state_bins + (self.env.action_space.n,))\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if self.prng.random_sample() < self.epsilon:\n",
    "            return self.prng.randint(self.env.action_space.n)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "\n",
    "        # First calculate the ratios, then convert to bin indices\n",
    "        ratios = [(state[i] + abs(self.state_bounds[i][0])) / (self.state_bounds[i][1] - self.state_bounds[i][0]) for i\n",
    "                  in range(len(state))]\n",
    "        state_d = [int(round((self.state_bins[i] - 1) * ratios[i])) for i in range(len(state))]\n",
    "        state_d = [min(self.state_bins[i] - 1, max(0, state_d[i])) for i in range(len(state))]\n",
    "\n",
    "        return tuple(state_d)\n",
    "\n",
    "    def learn(self, done, state, action, reward, state_, action_):\n",
    "\n",
    "        # Get current Q(s, a)\n",
    "        q_value = self.q_table[state][action]\n",
    "\n",
    "        # Check if next state is terminal, get next Q(s', a')\n",
    "        if not done:\n",
    "            q_value_ = reward + self.gamma * self.q_table[state_][action_]\n",
    "        else:\n",
    "            q_value_ = reward\n",
    "\n",
    "        # Update current Q(s, a)\n",
    "        self.q_table[state][action] += self.alpha * (q_value_ - q_value)\n",
    "\n",
    "    def do_episode(self, config):\n",
    "\n",
    "        # Initial values\n",
    "        done = False\n",
    "        score_e = 0\n",
    "        step_e = 0\n",
    "\n",
    "        # Get epsilon for initial state\n",
    "        self.update_epsilon_step()\n",
    "\n",
    "        # Episodic decay (only after linear decay)\n",
    "        self.update_alpha_episode()\n",
    "        self.update_epsilon_episode()\n",
    "\n",
    "        # Get current state s, act based on s\n",
    "        state = self.discretize_state(self.env.reset())\n",
    "        action = self.act(state)\n",
    "\n",
    "        # Continue while not crashed\n",
    "        while not done:\n",
    "\n",
    "            # Show on screen\n",
    "            if config['VERBOSE'] > 1:\n",
    "                self.env.render()\n",
    "\n",
    "            # Update for other steps\n",
    "            self.update_alpha_step()\n",
    "            self.update_epsilon_step()\n",
    "\n",
    "            # Get next state s' and reward, act based on s'\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            state_ = self.discretize_state(state_)\n",
    "            action_ = self.act(state_)\n",
    "\n",
    "            # Learn\n",
    "            self.learn(done, state, action, reward, state_, action_)\n",
    "\n",
    "            # Set next state and action to current\n",
    "            state = state_\n",
    "            action = action_\n",
    "\n",
    "            # Increment score and steps\n",
    "            score_e += reward\n",
    "            step_e += 1\n",
    "            self.step += 1\n",
    "\n",
    "        # Append score\n",
    "        self.score.append(score_e)\n",
    "        self.score_100.append(score_e)\n",
    "        mean_score = np.mean(self.score_100)\n",
    "\n",
    "        # Increment episode\n",
    "        self.episode += 1\n",
    "\n",
    "        if config['VERBOSE'] > 0:\n",
    "            logger.info(f'[Episode {self.episode}] - score: {score_e:.2f}, steps: {step_e}, e: {self.epsilon:.4f}, '\n",
    "                        f'a: {self.alpha:.4f}, 100-score: {mean_score:.2f}.')\n",
    "\n",
    "    def update_alpha_step(self):\n",
    "\n",
    "        # Linear decay\n",
    "        if self.step <= self.alpha_steps and self.alpha_steps > 0:\n",
    "            self.alpha = self.alpha_start - self.step * (self.alpha_start - self.alpha_end) / self.alpha_steps\n",
    "\n",
    "    def update_epsilon_step(self):\n",
    "\n",
    "        # Linear decay\n",
    "        if self.step <= self.epsilon_steps and self.epsilon_steps > 0:\n",
    "            self.epsilon = self.epsilon_start - self.step * (self.epsilon_start - self.epsilon_end) / self.epsilon_steps\n",
    "\n",
    "    def update_alpha_episode(self):\n",
    "\n",
    "        # Exponential decay\n",
    "        if self.step > self.alpha_steps:\n",
    "            self.alpha *= self.alpha_decay\n",
    "\n",
    "    def update_epsilon_episode(self):\n",
    "\n",
    "        # Exponential decay\n",
    "        if self.step > self.epsilon_steps:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "class QAgent(SarsaAgent):\n",
    "\n",
    "    #Agent that makes use of Q-learning (off-policy TD control).\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "    def learn(self, done, state, action, reward, state_, action_=None):\n",
    "\n",
    "        # Get current Q(s, a)\n",
    "        q_value = self.q_table[state][action]\n",
    "\n",
    "        # Check if next state is terminal, get next maximum Q-value\n",
    "        if not done:\n",
    "            q_value_ = reward + self.gamma * max(self.q_table[state_])\n",
    "        else:\n",
    "            q_value_ = reward\n",
    "\n",
    "        # Update current Q(s, a)\n",
    "        self.q_table[state][action] += self.alpha * (q_value_ - q_value)\n",
    "\n",
    "    def do_episode(self, config):\n",
    "\n",
    "        # Initial values\n",
    "        done = False\n",
    "        score_e = 0\n",
    "        step_e = 0\n",
    "\n",
    "        # Episodic decay (only after linear decay)\n",
    "        self.update_alpha_episode()\n",
    "        self.update_epsilon_episode()\n",
    "\n",
    "        # Get current state s\n",
    "        state = self.discretize_state(self.env.reset())\n",
    "\n",
    "        # Continue while not crashed\n",
    "        while not done:\n",
    "\n",
    "            # Show on screen\n",
    "            if config['VERBOSE'] > 1:\n",
    "                self.env.render()\n",
    "\n",
    "            # Get learning parameters\n",
    "            self.update_alpha_step()\n",
    "            self.update_epsilon_step()\n",
    "\n",
    "            # Act based on current state s\n",
    "            action = self.act(state)\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            state_ = self.discretize_state(state_)\n",
    "\n",
    "            # Learn\n",
    "            self.learn(done, state, action, reward, state_)\n",
    "\n",
    "            # Set next state to current\n",
    "            state = state_\n",
    "\n",
    "            # Increment score and steps\n",
    "            score_e += reward\n",
    "            step_e += 1\n",
    "            self.step += 1\n",
    "\n",
    "        # Append score\n",
    "        self.score.append(score_e)\n",
    "        self.score_100.append(score_e)\n",
    "        mean_score = np.mean(self.score_100)\n",
    "\n",
    "        # Increment episode\n",
    "        self.episode += 1\n",
    "\n",
    "        if config['VERBOSE'] > 0:\n",
    "            logger.info(f'[Episode {self.episode}] - score: {score_e:.2f}, steps: {step_e}, e: {self.epsilon:.4f}, '\n",
    "                        f'a: {self.alpha:.4f}, 100-score: {mean_score:.2f}.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from bottleneck import move_mean\n",
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_plots():\n",
    "\n",
    "    # Interactive, style\n",
    "    plt.ion()\n",
    "\n",
    "    # Get figure and subplots\n",
    "    figure = plt.figure(figsize=(10, 5))\n",
    "    axes = [figure.add_subplot(111)]  # 121), figure.add_subplot(122)]\n",
    "\n",
    "    # Configure subplots\n",
    "    axes[0].set_xlabel('episode [-]')\n",
    "    axes[0].set_ylabel('score [-]')\n",
    "    axes[0].set_title('Score moving average', fontstyle='italic')\n",
    "\n",
    "    # Get lines\n",
    "    lines = [axes[0].plot([0])[0]]\n",
    "\n",
    "    return figure, axes, lines\n",
    "\n",
    "\n",
    "def update_plots(figure, axes, lines, episode, score):\n",
    "\n",
    "    # Moving average\n",
    "    score_ma = move_mean(score, window=(100 if len(score) > 99 else len(score)), min_count=1)\n",
    "\n",
    "    # Update plot\n",
    "   \n",
    "    lines[0].set_data(range(1, episode + 1), score_ma)\n",
    "\n",
    "    # Rescale axes\n",
    "    for ax in axes:\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "\n",
    "    # Update figure\n",
    "    figure.tight_layout()\n",
    "    figure.canvas.draw()\n",
    "    figure.canvas.flush_events()\n",
    "\n",
    "    return figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "from gym import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = open('config/config_qlearn.yaml','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(config_file,Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ENV_ID': 'LunarLander-v2',\n",
       " 'ENV_SEED': 0,\n",
       " 'AGENT': 'qlearn',\n",
       " 'RECORD_DIR': 'record/qlearn/',\n",
       " 'CHECKPOINT_DIR': '',\n",
       " 'EPISODES': 8000,\n",
       " 'SAVE_EVERY': 100,\n",
       " 'STATE_BINS': [5, 5, 5, 5, 5, 5, 2, 2],\n",
       " 'STATE_BOUNDS': [[-1.0, 1.0],\n",
       "  [-1.0, 1.0],\n",
       "  [-1.0, 1.0],\n",
       "  [-1.0, 1.0],\n",
       "  [-1.0, 1.0],\n",
       "  [-1.0, 1.0],\n",
       "  [-1.0, 1.0],\n",
       "  [-1.0, 1.0]],\n",
       " 'VERBOSE': 1,\n",
       " 'CONTINUE': False,\n",
       " 'E_GREEDY': [1.0, 0.05, '1e5', 0.99],\n",
       " 'LEARNING_RATE': [0.2, 0.2, 0, 1],\n",
       " 'DISCOUNT_RATE': 0.97}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(config['RECORD_DIR']):\n",
    "    os.makedirs(config['RECORD_DIR'])\n",
    "if config['VERBOSE'] > 0:\n",
    "    figure, axes, lines = prepare_plots()\n",
    "else:\n",
    "    figure, axes, lines = None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAgent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "while agent.episode < agent.episode_count:\n",
    "\n",
    "        # Do episode\n",
    "        agent.do_episode(config)\n",
    "\n",
    "        # Update plots\n",
    "        if config['VERBOSE'] > 0:\n",
    "            figure = update_plots(figure, axes, lines, agent.episode, agent.score)\n",
    "\n",
    "        # Save every nth episode\n",
    "        if agent.episode % config['SAVE_EVERY'] == 0 and config['VERBOSE'] > 0:\n",
    "            agent.save_checkpoint(config)\n",
    "            figure.savefig(config['RECORD_DIR'] + 'score.pdf')\n",
    "\n",
    "        # Break when goal of 100-score > 200 is reached\n",
    "        if np.mean(agent.score_100) >= 200.0:\n",
    "            if config['VERBOSE'] > 0:\n",
    "                agent.save_checkpoint(config)\n",
    "                figure.savefig(config['RECORD_DIR'] + 'score.pdf')\n",
    "            logger.info('Goal reached!')\n",
    "            break\n",
    "\n",
    "# Close\n",
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
